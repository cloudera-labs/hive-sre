processes:
  - type:                     "dbSet"
    id:                       1
    active:                   true
    displayName:              "Locations Scan"
    queryDefinitionReference: "/hive_u3_queries.yaml"
    dbListingQuery:           "db_tbl_count"
    listingColumns: [ "name" ,"tbl_name" , "tbl_type" ,"part_name" , "path_location" ]
    pathsListingQuery:        "tbl_part_locations"
    commandChecks:
      - displayName:        "Hive 3 Upgrade Check - Missing Directory Locations Scan"
        title:              "# Hive 3 Upgrade Check - Missing Direcotories (v.${Implementation-Version})\n\n## Missing Directory Locations Scan"
        note:               "
         *Remediation Options*\n
                _NOTE:_ Beware of \"Smart Quote\" AND other conversions via Markdown Renderers. Copy/paste from 'raw' text to ensure compatibility.\n\n
                - Rendered text may not work in 'Hive' or 'Hadoop CLI'\n
                - Parse either of these options and run in 'hive' or 'hive-sre-cli' (alias for [hadoopcli](https://github.com/dstreev/hadoop-cli) )\n
                \n
         These results represent tables or partitions that do NOT have a directory on the File System.\n\n
         These missing directories will prevent the post upgrade (Hive 1 to 3) process from completing and must be addressed.
         There are two options for addressing this, 'drop' the table/partition definition OR create the missing directory.
         Both options are listed here.\n\n
         We recommend, were possible, to 'drop' the table or partition.\n\n
         Like other reports, this should be run far in advance of the upgrade.  Metastore hygiene is
         always a good practice.\n\n
         This process should be run frequently, up to the point of the upgrade, to ensure all items have been addressed.  Before
         the upgrade can succeed, this report should be *empty*.\n\n"
        header:             "| DB.Table:Partition Spec | Hive SQL (recommended) | HDFS |\n|:---|:---|:---|"
        errorDescription:   "Scan Actions"
        successDescription: "Issues"
        errorFilename:      loc_scan_missing_dirs.md
        successFilename:    loc_scan_missing_dirs_issue.txt
        invertCheck:        false
        pathCommand:        "lsp -f path -self -t \"%5$s\""
        reportOnResults:    false
        reportOnPath:       true
        processOnError:     true
        processOnSuccess:   false
        checkCalculations:
          partitionCheck:
            RECORDS:
              test: "if (\"%4$s\".equals(\" \")) true; else false;"
              pass: "\"| %1$s.%2$s | DROP TABLE IF EXISTS `%1$s`.`%2$s`; | mkdir -p \\\"%5$s\\\" |\""
              fail: "\"| %1$s.%2$s:\" + com.cloudera.utils.hive.sre.Utils.dirToPartitionSpec('%4$s') + \" | ALTER TABLE `%1$s`.`%2$s` DROP IF EXISTS PARTITION (\" + com.cloudera.utils.hive.sre.Utils.dirToPartitionSpec('%4$s') + \"); | mkdir -p \\\"%5$s\\\" |\""
              params: [ "hive" ]

  - type:                     "dbSet"
    id:                       2
    active:                   true
    displayName:              "Bad ACID ORC Filenames"
    queryDefinitionReference: "/hive_u3_queries.yaml"
    dbListingQuery:           "db_tbl_count"
    listingColumns: [ "db_name" ,"tbl_name" , "tbl_type" ,"part_name" , "path_check" ]
    pathsListingQuery:        "tbl_mngd_non_acid_locations"
    commandChecks:
      - displayName:          "Hive 3 Upgrade Check - Bad Filename Format for ACID ORC Conversion"
        title:                "# Hive 3 Upgrade Check - Bad ORC Filenames (v.${Implementation-Version})\n\n## Bad Filename Format for ACID ORC Conversion"
        header:               "| DB.Table:Partition | Path | Filename |\n|:---|:---|:---|"
        note:                 "
        > File naming conventions that could prevent a 'managed' ORC table from transitioning to an 'ACID' table\n\n
        Tables that will be converted to ACID tables, require filenames to match a very specific pattern.  If files in the table(partition)
        directories do not meet this pattern, the conversion WILL fail.\n\n
        There are two options for handling this:\n
        - Convert the table to 'EXTERNAL' so they will NOT be converted.  These tables are listed in the *Managed Non-ACID to ACID Table Migrations*
        report.\n
        - Perform a classic 'INSERT OVERWRITE TABLE <self> FROM <self>'.  This exercise will force 'Hive' to rewrite the table data
        back into the same directories.  The process of _rewriting_ will ensure the filenames match the expected patterns.\n
        If the tables are converted to 'EXTERNAL', they will fall off this report when run again.\n\n"
        invertCheck:          true
        errorDescription:     "Bad ACID Filenames"
        successDescription:   "Bad ACID Filename Issues"
        errorFilename:        "bad_filenames_for_orc_conversion.md"
        successFilename:      "bad_filename_issues.txt"
        pathCommand:          "lsp -R -F \"([0-9]+_[0-9]+)|([0-9]+_[0-9]_copy_[0-9]+)|(_orc_acid_version)|(bucket_\\d+(_\\d+)?)|(_metadata_acid)\" -i -Fe file -v -f parent,file \"%5$s\""
        onErrorRecordCommand: "| %1$s.%2$s:%4$s | %6$s | %7$s |"
        reportOnResults:      true
        reportOnPath:         true
        processOnError:       true
        processOnSuccess:     false
  - type:                     "dbSet"
    id:                       3
    active:                   true
    displayName:              "Managed Non-ACID to ACID Table Migrations"
    queryDefinitionReference: "/hive_u3_queries.yaml"
    errorDescription:         "Issues"
    successDescription:       "SQL Conversion Scipt"
    errorFilename:            "managed_upgrade_2_acid_issues.txt"
    successFilename:          "managed_upgrade_2_acid.sql"
    dbListingQuery:           "db_tbl_count"
    listingColumns:           [ "db_name" ,"tbl_name" , "tbl_type" ,"tbl_location" ]
    pathsListingQuery:        "managed_2_acid"
    hsmmElements:
      databaseField:          "db_name"
      tableField:             "tbl_name"
    commandChecks:
      - displayName:            "Hive 3 Upgrade Check - Potential ACID Conversions"
        title:                  "-- Managed Non - ACID to ACID Table Migrations  (v.${Implementation-Version})\n"
        note:                   "
        -- This report highlights classic Managed Non-ACID tables that _could_ be converted to ACID tables by the post hive
        'strictmigration' process.  This will dramatically change the characteristics of the table and affect the integration
        methods used by Spark SQL, direct HDFS access, etc..  Many of which will 'fail' and/or 'corrupt' the newly converted
        Hive ACID table.\n

        -- We recommend that you proactively force these tables to 'EXTERNAL' tables (with purge characteristics) to prevent this
        conversion.  Changing tables to ACID, post migration is much easier than trying to revert converted ACID tables back
        to non-ACID tables.\n

        -- Tables that are managed and aren't converted by this program, will be converted to 'EXTERNAL/purge' when they don't
        meet all the criteria for the conversion, regardless.  Using the 'action' elements of this report simply ensures
        you are in control of the migration process.\n\n"
        invertCheck:            false
        pathCommand:            "lsp -f user -self \"%4$s\""
        onSuccessPathCommand:   "\"Rewrite database table: %1$s.%2$s [Partition: \" + com.cloudera.utils.hive.sre.Utils.dirToPartitionSpec('%4$s') + \"]"
        onSuccessRecordCommand: "# Bad filename %2$s in directory: %1$s "
        reportOnResults:        false
        reportOnPath:           false
        processOnError:         false
        processOnSuccess:       true
        checkCalculations:
          ownershipCheck:
            RECORDS:
              test: "if (\"%5$s\".equals(\"%6$s\")) true; else false;"
              pass: "\"-- Table is owned by '%5$s' and not currently ACID.\\n
                    -- This table will be migrated unless changed.\\n
                    ALTER TABLE %1$s.%2$s SET TBLPROPERTIES('EXTERNAL'='TRUE', 'external.table.purge'='true')\\;\""
              fail: "\"-- Table is owned by '%5$s', not '%6$s', and NOT currently ACID.\\n
                    -- This table 'could' be migrated to an ACID table unless changed.\\n
                    -- Recommend forcing the manual conversion to ensure table isn't inadvertently migrated.\\n
                    ALTER TABLE %1$s.%2$s SET TBLPROPERTIES('EXTERNAL'='TRUE', 'external.table.purge'='true')\\;\""
              params: [ "hive" ]
  - type:                     "dbSet"
    id:                       4
    active:                   true
    displayName:              "Compaction Check"
    queryDefinitionReference: "/hive_u3_queries.yaml"
    errorDescription:         "Issues"
    successDescription:       "Compaction SQL Script"
    errorFilename:            "managed_compactions_issues.txt"
    successFilename:          "managed_compactions.sql"
    dbListingQuery:           "db_tbl_count"
    listingColumns: [ "name" ,"tbl_name" , "tbl_type" ,"part_name", "path_location" ]
    pathsListingQuery:        "managed_tbl_locations"
    commandChecks:
      - displayName:      "-- Hive 3 Upgrade Checks -- Compaction Check"
        title:            "-- Hive 3 Upgrades - Compaction Check (v.${Implementation-Version})\n"
        note:             "
        -- This process reviews existing ACID tables for 'delta' directories.  'Delta' directories for ACID v1 are NOT compatible
        with ACID v2 in Hive 3.  The _delta's_ must be compacted *BEFORE* the upgrade occurs.\n

        -- ANY *delta* directories that are NOT compacted *SUCCESSFULLY* before the upgrade will NOT be compatible after the upgrade
        and the data will be LOST.\n

        -- The 'actions' listed in the report will issue the compaction commands to do this.  Be sure the compactions run *SUCCESSFULLY*
        before upgrading.\n

        -- We suggested running this report and issue the *action's* well before the upgrade.  The compactor _may_ require tuning
        to complete these actions in a reasonable amount of time.\n

        -- Run the report several times before the upgrade, with the goals of eliminating any items on this report BEFORE starting
        the upgrade.\n\n"
        invertCheck:      false
        pathCommand:      "lsp -R -F .*delta_.* -t -sp -f path \"%5$s\""
        #        onSuccessPathCommand: "ALTER TABLE %1$s.%2$s COMPACT 'MAJOR'; TODO: Need to account for Partitions."
        reportOnResults:  false
        reportOnPath:     true
        processOnError:   false
        processOnSuccess: true
        checkCalculations:
          partitionCheck:
            PATH:
              test: "if (\"%4$s\".trim().length() == 0) true; else false;"
              pass: "\"ALTER TABLE %1$s.%2$s COMPACT 'MAJOR';\""
              fail: "\"ALTER TABLE %1$s.%2$s PARTITION (\" + com.cloudera.utils.hive.sre.Utils.dirToPartitionSpec('%4$s') + \") COMPACT \\\"MAJOR\\\"\\;\""
              params: [ "hive" ]
  - type:                     "metastore.report"
    id:                       5
    active:                   true
    displayName:              "Hive Metastore Checks"
    title:                    "# Hive Metastore Checks  (v.${Implementation-Version})"
    queryDefinitionReference: "/hive_u3_queries.yaml"
    errorDescription:         "Processing Issues"
    successDescription:       "Hive Metastore"
    errorFilename:            hms_checks_err.txt
    successFilename:          hms_checks.md
    metastoreQueryDefinitions:
      - query:                       "questionable_serdes"
        listingColumns: [ "db_name" ,"tbl_name", "tbl_serde_slib" ]
        resultMessageHeader:         "## Questionable Serde's\n
                              > Listed tables should be review to ensure the Serde is still available.\n
                              Missing Serde's can disrupt a Hive Upgrade/Migration Process\n"
        resultMessageDetailHeader:   "| Database | Table | Serde |\n|:---|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s | %3$s |"
      - query:                       "legacy_kudu_storage_handle_class"
        listingColumns: [ "db_name" ,"tbl_name"]
        resultMessageHeader:         "\n## Legacy Kudu Serde's\n
                              > Listed tables need the Kudu Serde Class upgraded.\n
                              "
        resultMessageDetailHeader:   "| Database | Table |\n|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s |"
        hsmmElements:
          databaseField: "db_name"
          tableField:    "tbl_name"
      - query:                       "legacy_decimal_defs_orc"
        listingColumns: [ "name" ,"tbl_name", "location", "column_name", "type_name", "integer_idx" ]
        resultMessageHeader: "\n## Legacy DECIMAL Definitions in ORC Tables\n\n
                              **EXPLANATION**\n
                              Introduced with Hive 0.11.0 - Decimal Fields defined in Hive 0.11.0 -> 0.12.0 will NOT have ‘precision’ or ‘scale’ defined.\n\n
                              Legacy Hive (0.11.0->0.12.0)\n\n
                              Limited to 38 digits\n
                              Post 0.12.0 Behavior with NO conversion\n\n
                              Default precision is ‘10’\n
                              Default Scale is ‘0’\n\n
                              **IMPACT**\n\n
                              For HIVE, this behavior, if not adjusted, will lead to changes in ‘results’ after upgrading to Hive 0.13.0 and above.\n
                              For Spark, this behavior, if NOT corrected, will result in errors. These tables definitions need to be fixed in the metastore to match the data structure in the ORC file.\n
                              This will affect many mature deployments where Hive and Spark have been in heavy usage for a long time. This has been observed in a HDP 2.6.5 to CDP 7.1.4 upgrade. Need confirmation from engineering about extent of impact across versions.\n\n
                              **PRE-UPGRADE STEPS REQUIRED**\n\n
                              For tables defined in Hive 0.11.0/0.12.0 with DECIMAL type\n\n
                              First we need to see which columns differ on their type. To find out execute:\n\n
                              `hive --orcfiledump /hdfsPathWhereTheORCFileOFTheTableSits/MyORCFile` \n\n
                              There we will see the dump of the internal schema of the table:\n\n
                              ```\n
                              File Version: 0.12 with ORC_135\n
                              Rows: 62079\n
                              Compression: ZLIB\n
                              Compression size: 262144\n
                              Calendar: Julian/Gregorian\n
                              Type: struct<jobno:double,articlenumber:decimal(20,0),jobdesc:string,businessunit:string,quantityordered:decimal(7,0),quantitydelivered:decimal(7,0),status:string,working:string,pages:int,impressions:decimal(7,0),jobtypedesc:string,type:string,orderdate:string,requiredtime:string,lastopdesc:string,load_date:date>\n
                              ```\n\n
                              Check the columns where the definition on ORC does not match the Hive definition, and alter these to match.\n\n
                              In the table example we have from the ORC that was provided, we should do in beeline:\n\n
                              ```\n
                              alter table testorc change column articlenumber articlenumber decimal(20,0);\n
                              ```\n\n
                              Do this for EACH COLUMN affected\n\n"
        resultMessageDetailHeader:   "| Database | Table | Location | Column | Type | Idx |\n|:---|:---|:---|:---|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s | %3$s | %4$s | %5$s | %6$s |"
      - query:                       "managed_tbl_shadows"
        listingColumns: [ "db_name" ,"tbl_name", "tbl_location" ]
        resultMessageHeader:         "\n## Managed Shadow Tables"
        resultMessageDetailHeader:   "| Database | Table | Path |\n|:---|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s | %3$s |"
      - query:                       "db_tbl_part_count"
        listingColumns: [ "name" ,"tbl_count", "part_count" ]
        resultMessageHeader:         "\n## Table Partition Count"
        resultMessageDetailHeader:   "| Database | Tables | Partitions |\n|:---|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s | %3$s |"
