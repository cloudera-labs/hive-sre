# Copyright 2021 Cloudera, Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
module: "u3"
processes:
  - type:                     "dbSet"
    id:                       1
    active:                   true
    displayName:              "Locations Scan"
    queryDefinitionReference: "/hive_u3_queries.yaml"
    dbListingQuery:           "db_tbl_count"
    listingColumns: [ "name" ,"tbl_name" , "tbl_type" ,"part_name" , "path_location", "part_id" ]
    pathsListingQuery:        "tbl_part_locations"
    commandChecks:
      - displayName:        "Hive 3 Upgrade Check - Missing Directory Locations Scan"
        title:              "# Hive 3 Upgrade Check - Missing Directories (v.${Implementation-Version})\n\n## Missing Directory Locations Scan"
        note:               "
         *Remediation Options*\n
                _NOTE:_ Beware of \"Smart Quote\" AND other conversions via Markdown Renderers. Copy/paste from 'raw' text to ensure compatibility.\n\n
                - Rendered text may not work in 'Hive' or 'Hadoop CLI'\n
                - Parse either of these options and run in 'hive' or 'hive-sre-cli' (alias for [hadoopcli](https://github.com/dstreev/hadoop-cli) )\n
                \n
         These results represent tables or partitions that do NOT have a directory on the File System.\n\n
         These missing directories will prevent the post upgrade (Hive 1 to 3) process from completing and must be addressed.
         There are two options for addressing this, 'drop' the table/partition definition OR create the missing directory.
         Both options are listed here.\n\n
         We recommend, were possible, to 'drop' the table or partition.\n\n
         Like other reports, this should be run far in advance of the upgrade.  Metastore hygiene is
         always a good practice.\n\n
         This process should be run frequently, up to the point of the upgrade, to ensure all items have been addressed.  Before
         the upgrade can succeed, this report should be *empty*.\n\n"
        header:             "| DB.Table:Partition Spec | Hive SQL (recommended) | HDFS | Hive MSCK (Post Upgrade) | PART ID |\n|:---|:---|:---|:---|:---|"
        errorDescription:   "Scan Actions"
        successDescription: "Issues"
        errorFilename:      loc_scan_missing_dirs.md
        successFilename:    loc_scan_missing_dirs_issue.txt
        invertCheck:        false
        pathCommand:        "lsp -f path -self -t \"%5$s\""
        reportOnResults:    false
        reportOnPath:       true
        processOnError:     true
        processOnSuccess:   false
        checkCalculations:
          partitionCheck:
            RECORDS:
              test: "if (\"%4$s\".equals(\" \")) true; else false;"
              pass: "\"| %1$s.%2$s | DROP TABLE IF EXISTS `%1$s`.`%2$s`; | mkdir -p \\\"%5$s\\\" | MSCK REPAIR TABLE `%1$s`.`%2$s` SYNC PARTITIONS; | |\""
              fail: "\"| %1$s.%2$s:\" + com.cloudera.utils.hive.sre.Utils.dirToPartitionSpec('%4$s') + \" | ALTER TABLE `%1$s`.`%2$s` DROP IF EXISTS PARTITION (\" + com.cloudera.utils.hive.sre.Utils.dirToPartitionSpec('%4$s') + \"); | mkdir -p \\\"%5$s\\\" | | %6$s |\""
              params: [ "hive" ]

  - type:                     "dbSet"
    id:                       2
    active:                   true
    displayName:              "Bad ACID ORC Filenames"
    queryDefinitionReference: "/hive_u3_queries.yaml"
    dbListingQuery:           "db_tbl_count"
    listingColumns: [ "db_name" ,"tbl_name" , "tbl_type" ,"part_name" , "path_check" ]
    pathsListingQuery:        "tbl_mngd_non_acid_locations"
    commandChecks:
      - displayName:          "Hive 3 Upgrade Check - Bad Filename Format for ACID ORC Conversion"
        title:                "# Hive 3 Upgrade Check - Bad ORC Filenames (v.${Implementation-Version})\n\n## Bad Filename Format for ACID ORC Conversion"
        header:               "| DB.Table:Partition | Path | Filename |\n|:---|:---|:---|"
        note:                 "
        > File naming conventions that could prevent a 'managed' ORC table from transitioning to an 'ACID' table\n\n
        Tables that will be converted to ACID tables, require filenames to match a very specific pattern.  If files in the table(partition)
        directories do not meet this pattern, the conversion WILL fail.\n\n
        There are two options for handling this:\n
        - Convert the table to 'EXTERNAL' so they will NOT be converted.  These tables are listed in the *Managed Non-ACID to ACID Table Migrations*
        report.\n
        - Perform a classic 'INSERT OVERWRITE TABLE <self> FROM <self>'.  This exercise will force 'Hive' to rewrite the table data
        back into the same directories.  The process of _rewriting_ will ensure the filenames match the expected patterns.\n
        If the tables are converted to 'EXTERNAL', they will fall off this report when run again.\n\n"
        invertCheck:          true
        errorDescription:     "Bad ACID Filenames"
        successDescription:   "Bad ACID Filename Issues"
        errorFilename:        "bad_filenames_for_orc_conversion.md"
        successFilename:      "bad_filename_issues.txt"
        pathCommand:          "lsp -R -F \"([0-9]+_[0-9]+)|([0-9]+_[0-9]_copy_[0-9]+)|(_orc_acid_version)|(bucket_\\d+(_\\d+)?)|(_metadata_acid)\" -i -Fe file -v -f parent,file \"%5$s\""
        onErrorRecordCommand: "| %1$s.%2$s:%4$s | %6$s | %7$s |"
        reportOnResults:      true
        reportOnPath:         true
        processOnError:       true
        processOnSuccess:     false
  - type:                     "metastore.report"
    id:                       3
    active:                   true
    displayName:              "Legacy Managed Non-Transactional (non-acid) Conversions to EXTERNAL/PURGE"
    title:                    "# Legacy Managed Non-Transactional (non-acid) Conversions to EXTERNAL/PURGE (v.${Implementation-Version})"
    queryDefinitionReference: "/hive_u3_queries.yaml"
    errorDescription:         "Processing Issues"
    successDescription:       "Hive Metastore"
    errorFilename:            "legacy_managed_upgrade_issues.txt"
    successFilename:          "legacy_managed_upgrade.md"
    metastoreQueryDefinitions:
      - query:                       "legacy_managed"
        listingColumns: [ "name" ,"tbl_name" , "created_date", "tbl_type" ,"location" ]
        resultMessageHeader:         "
          This is a list of tables that meet the LEGACY Managed non-transactional criteria.  Use this to understand the scope of the change.
            \n\n
            > Run the `u3e` process to execute the fixes for this list of tables.
              \n\n"
        resultMessageDetailHeader:   "| Database | Table | Create Date | Type | Location |\n|:---|:---|:---|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s | %3$s | %4$s | %5$s |"
  - type:                     "dbSet"
    id:                       4
    active:                   true
    displayName:              "Compaction Check"
    queryDefinitionReference: "/hive_u3_queries.yaml"
    errorDescription:         "Issues"
    successDescription:       "Compaction SQL Script"
    errorFilename:            "managed_compactions_issues.txt"
    successFilename:          "managed_compactions.sql"
    dbListingQuery:           "db_tbl_count"
    listingColumns: [ "name" ,"tbl_name" , "tbl_type" ,"part_name", "path_location" ]
    pathsListingQuery:        "managed_tbl_locations"
    commandChecks:
      - displayName:      "-- Hive 3 Upgrade Checks -- Compaction Check"
        title:            "-- Hive 3 Upgrades - Compaction Check (v.${Implementation-Version})\n"
        note:             "
        -- This process reviews existing ACID tables for 'delta' directories.  'Delta' directories for ACID v1 are NOT compatible
        with ACID v2 in Hive 3.  The _delta's_ must be compacted *BEFORE* the upgrade occurs.\n

        -- ANY *delta* directories that are NOT compacted *SUCCESSFULLY* before the upgrade will NOT be compatible after the upgrade
        and the data will be LOST.\n

        -- The 'actions' listed in the report will issue the compaction commands to do this.  Be sure the compactions run *SUCCESSFULLY*
        before upgrading.\n

        -- We suggested running this report and issue the *action's* well before the upgrade.  The compactor _may_ require tuning
        to complete these actions in a reasonable amount of time.\n

        -- Run the report several times before the upgrade, with the goals of eliminating any items on this report BEFORE starting
        the upgrade.\n\n"
        invertCheck:      false
        pathCommand:      "lsp -R -F .*delta_.* -t -sp -f path \"%5$s\""
        #        onSuccessPathCommand: "ALTER TABLE %1$s.%2$s COMPACT 'MAJOR'; TODO: Need to account for Partitions."
        reportOnResults:  false
        reportOnPath:     true
        processOnError:   false
        processOnSuccess: true
        checkCalculations:
          partitionCheck:
            PATH:
              test: "if (\"%4$s\".trim().length() == 0) true; else false;"
              pass: "\"ALTER TABLE `%1$s`.`%2$s` COMPACT 'MAJOR';\""
              fail: "\"ALTER TABLE `%1$s`.`%2$s` PARTITION (\" + com.cloudera.utils.hive.sre.Utils.dirToPartitionSpec('%4$s') + \") COMPACT \\\"MAJOR\\\"\\;\""
              params: [ "hive" ]
  - type:                     "metastore.report"
    id:                       5
    active:                   true
    displayName:              "Hive Metastore Checks"
    title:                    "# Hive Metastore Checks  (v.${Implementation-Version})"
    queryDefinitionReference: "/hive_u3_queries.yaml"
    errorDescription:         "Processing Issues"
    successDescription:       "Hive Metastore"
    errorFilename:            hms_checks_err.txt
    successFilename:          hms_checks.md
    metastoreQueryDefinitions:
      - query:                       "questionable_serdes"
        listingColumns: [ "db_name" ,"tbl_name", "tbl_serde_slib" ]
        resultMessageHeader:         "## Questionable Serde's\n
                              > Listed tables should be reviewed to ensure the Serde is still available.\n
                              Missing Serde's can disrupt a Hive Upgrade/Migration Process\n"
        resultMessageDetailHeader:   "| Database | Table | Serde |\n|:---|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s | %3$s |"
      - query:                       "legacy_kudu_storage_handle_class"
        listingColumns: [ "db_name" ,"tbl_name"]
        resultMessageHeader:         "\n## Legacy Kudu Serde's\n
                              > List of tables that need the Kudu Serde Class upgraded.\n
                              "
        resultMessageDetailHeader:   "| Database | Table |\n|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s |"
        hsmmElements:
          databaseField: "db_name"
          tableField:    "tbl_name"
      - query:                       "legacy_decimal_defs_orc"
        listingColumns: [ "name" ,"tbl_name", "location", "column_name", "type_name", "integer_idx" ]
        resultMessageHeader: "\n## Legacy DECIMAL Definitions in ORC Tables\n\n
                              **EXPLANATION**\n
                              Introduced with Hive 0.11.0 - Decimal Fields defined in Hive 0.11.0 -> 0.12.0 will NOT have ‘precision’ or ‘scale’ defined.\n\n
                              Legacy Hive (0.11.0->0.12.0)\n\n
                              Limited to 38 digits\n
                              Post 0.12.0 Behavior with NO conversion\n\n
                              Default precision is ‘10’\n
                              Default Scale is ‘0’\n\n
                              **IMPACT**\n\n
                              For HIVE, this behavior, if not adjusted, will lead to changes in ‘results’ after upgrading to Hive 0.13.0 and above.\n
                              For Spark, this behavior, if NOT corrected, will result in errors. These tables definitions need to be fixed in the metastore to match the data structure in the ORC file.\n
                              This will affect many mature deployments where Hive and Spark have been in heavy usage for a long time. This has been observed in a HDP 2.6.5 to CDP 7.1.4 upgrade. Need confirmation from engineering about extent of impact across versions.\n\n
                              **PRE-UPGRADE STEPS REQUIRED**\n\n
                              For tables defined in Hive 0.11.0/0.12.0 with DECIMAL type\n\n
                              First we need to see which columns differ on their type. To find out execute:\n\n
                              `hive --orcfiledump /hdfsPathWhereTheORCFileOFTheTableSits/MyORCFile` \n\n
                              There we will see the dump of the internal schema of the table:\n\n
                              ```\n
                              File Version: 0.12 with ORC_135\n
                              Rows: 62079\n
                              Compression: ZLIB\n
                              Compression size: 262144\n
                              Calendar: Julian/Gregorian\n
                              Type: struct<jobno:double,articlenumber:decimal(20,0),jobdesc:string,businessunit:string,quantityordered:decimal(7,0),quantitydelivered:decimal(7,0),status:string,working:string,pages:int,impressions:decimal(7,0),jobtypedesc:string,type:string,orderdate:string,requiredtime:string,lastopdesc:string,load_date:date>\n
                              ```\n\n
                              Check the columns where the definition on ORC does not match the Hive definition, and alter these to match.\n\n
                              In the table example we have from the ORC that was provided, we should do in beeline:\n\n
                              ```\n
                              alter table testorc change column articlenumber articlenumber decimal(20,0);\n
                              ```\n\n
                              Do this for EACH COLUMN affected\n\n"
        resultMessageDetailHeader:   "| Database | Table | Location | Column | Type | Idx |\n|:---|:---|:---|:---|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s | %3$s | %4$s | %5$s | %6$s |"
      - query:                       "managed_tbl_shadows"
        listingColumns: [ "db_name" ,"tbl_name", "tbl_location" ]
        resultMessageHeader:         "\n## Managed Shadow Tables\n\nThese are current managed non-transactional tables that under 
        certain conditions could be converted to ACID tables by HSMM.  When two tables share a location for data, those tables can NOT 
        be ACID tables.  Hence, these intersections could cause issues converting these tables to ACID tables.\n"
        resultMessageDetailHeader:   "| Database | Table | Path |\n|:---|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s | %3$s |"
      - query:                       "db_tbl_part_count"
        listingColumns: [ "name" ,"tbl_count", "part_count" ]
        resultMessageHeader:         "\n## Table Partition Count"
        resultMessageDetailHeader:   "| Database | Tables | Partitions |\n|:---|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s | %3$s |"
  - type:                     "metastore.report"
    id:                       6
    active:                   true
    displayName:              "Hive ACID tables missing 'bucketing_version'='2' tblproperties"
    title:                    "# Hive ACID tables missing 'bucketing_version'='2' tblproperties  (v.${Implementation-Version})"
    queryDefinitionReference: "/hive_u3_queries.yaml"
    errorDescription:         "Issues"
    successDescription:       "SQL Conversion Script"
    errorFilename:            "acid_bucketing_update_issues.txt"
    successFilename:          "acid_bucketing_update.md"
    metastoreQueryDefinitions:
      - query: "acid_bucket_report"
        listingColumns: [ "db_name" ,"tbl_name" ]
        resultMessageHeader: "
           This report will show the ACID tables that don't contain the required 'bucketing_version=2' tblproperties configuration.\n
           Without this setting, you will have issues access/reading/processing the upgraded ACID table.\n
           We recommend that you run the `u3e` process to correct after the upgrade.\n\n
        "
        resultMessageDetailHeader: "| Database | Table |\n|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s |"
  - type:                     "metastore.report"
    id:                       7
    active:                   true
    displayName:              "Hive Storage Handlers"
    title:                    "# Hive Storage Handlers  (v.${Implementation-Version})\n\n
    If any of these storage handlers show up as a 'MANAGED_TABLE' type and you are upgrading from a legacy (Hive 1/2) platform, 
    the 'Managed Non-ACID to ACID Table Migrations' process will pick them up for conversion.  If the storage handlers location 
    information is NOT valid or the Hive table is orphaned, the upgrade will fail while attempting to convert the table and 
    affect your ability to complete the upgrade process for Hive.\n\n Ensure the location for 'MANAGED_TABLE' Storage Handler backed 
    tables is valid AND available.\n\n"
    queryDefinitionReference: "/hive_u3_queries.yaml"
    errorDescription:         "Processing Issues"
    successDescription:       "Hive Storage Handlers"
    errorFilename:            hms_storage_handlers_err.txt
    successFilename:          hms_storage_handlers.md
    metastoreQueryDefinitions:
      - query:               "storage_format"
        parameters:
          tableparam:
            override: "kudu.table_name"
            sqlType:  12
            location: 1
          inputformat:
            override: "org.apache.hadoop.hive.kudu.KuduInputFormat"
            sqlType:  12
            location: 2
        listingColumns: [ "name" ,"tbl_name", "tbl_type", "param_value", "create_date" ]
        resultMessageHeader:         "## Kudu Storage Handler\n
                              > List of tables using Kudu for Storage.\n
                              > Manage tables in this report will be picked up by the 'Managed Non-ACID to ACID Table Migrations'\n
                              - If the location or source is NOT available, converting the table to EXTERNAL will fail.\n
                              \nWe suggest running the [`kudu`](https://kudu.apache.org/docs/hive_metastore.html) utility to clean up abandon tables
                              in the metastore.\n\n
                              "
        resultMessageDetailHeader:   "| Database | Table | Type | Location | Create Date |\n|:---|:---|:---|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s | %3$s | %4$s | %5$s |"
      - query:               "storage_format"
        parameters:
          tableparam:
            override: "hbase.table.name"
            sqlType:  12
            location: 1
          inputformat:
            override: "org.apache.hadoop.hive.hbase.HBaseStorageHandler"
            sqlType:  12
            location: 2
        listingColumns: [ "name" ,"tbl_name", "tbl_type", "param_value", "create_date" ]
        resultMessageHeader:  "## HBase Storage Handler\n
                              > List of tables using HBase for Storage.\n
                              > Manage tables in this report will be picked up by the 'Managed Non-ACID to ACID Table Migrations'\n
                              - If the location or source is NOT available, converting the table to EXTERNAL will fail.\n\n
                              "
        resultMessageDetailHeader:   "| Database | Table | Type | Location | Create Date |\n|:---|:---|:---|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s | %3$s | %4$s | %5$s |"
      - query:               "storage_format"
        parameters:
          tableparam:
            override: "druid.zk.service.host"
            sqlType:  12
            location: 1
          inputformat:
            override: "org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat"
            sqlType:  12
            location: 2
        listingColumns: [ "name" ,"tbl_name", "tbl_type", "param_value", "create_date" ]
        resultMessageHeader:  "## Druid Storage Handler\n
                              > List of tables using Druid for Storage.\n
                              > Manage tables in this report will be picked up by the 'Managed Non-ACID to ACID Table Migrations'\n
                              - If the location or source is NOT available, converting the table to EXTERNAL will fail.\n\n
                              "
        resultMessageDetailHeader:   "| Database | Table | Type | Location | Create Date |\n|:---|:---|:---|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s | %3$s | %4$s | %5$s |"
      - query:               "storage_format"
        parameters:
          tableparam:
            override: "phoenix.hbase.table.name"
            sqlType:  12
            location: 1
          inputformat:
            override: "org.apache.phoenix.hive.HivePhoenixInputFormat"
            sqlType:  12
            location: 2
        listingColumns: [ "name" ,"tbl_name", "tbl_type", "param_value", "create_date" ]
        resultMessageHeader:  "## Phoenix Storage Handler\n
                              > List of tables using Phoenix for Storage.\n
                              > Manage tables in this report will be picked up by the 'Managed Non-ACID to ACID Table Migrations'\n
                              - If the location or source is NOT available, converting the table to EXTERNAL will fail.\n\n
                              "
        resultMessageDetailHeader:   "| Database | Table | Type | Location | Create Date |\n|:---|:---|:---|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s | %3$s | %4$s | %5$s |"
      - query:               "storage_format"
        parameters:
          tableparam:
            override: "hive.sql.jdbc.url"
            sqlType:  12
            location: 1
          inputformat:
            override: "org.apache.hive.storage.jdbc.JdbcInputFormat"
            sqlType:  12
            location: 2
        listingColumns: [ "name" ,"tbl_name", "tbl_type", "param_value", "create_date" ]
        resultMessageHeader:  "## JDBC Storage Handler\n
                              > List of tables using JDBC for Storage.\n
                              > Manage tables in this report will be picked up by the 'Managed Non-ACID to ACID Table Migrations'\n
                              - If the location or source is NOT available, converting the table to EXTERNAL will fail.\n\n
                              "
        resultMessageDetailHeader:   "| Database | Table | Type | Location | Create Date |\n|:---|:---|:---|:---|:---|"
        resultMessageDetailTemplate: "| %1$s | %2$s | %3$s | %4$s | %5$s |"
  - type:                     "dbSet"
    id:                       8
    active:                   false
    displayName:              "Managed Non-ACID to ACID Table Migrations"
    queryDefinitionReference: "/hive_u3_queries.yaml"
    errorDescription:         "Issues"
    successDescription:       "SQL Conversion Script"
    errorFilename:            "managed_upgrade_2_acid_issues.txt"
    successFilename:          "managed_upgrade_2_acid.sql"
    dbListingQuery:           "db_tbl_count"
    listingColumns:           [ "db_name" ,"tbl_name" , "created_date", "tbl_type" ,"tbl_location" ]
    pathsListingQuery:        "managed_2_acid"
    hsmmElements:
      databaseField:          "db_name"
      tableField:             "tbl_name"
    skipCommandCheck:
      displayName:            "Hive 3 Upgrade Check - Potential ACID Conversions (Skip Command Check)"
      title:                  "-- Managed Non - ACID to ACID Table Migrations  (v.${Implementation-Version})\n"
      note:                   "
      -- This report highlights classic Managed Non-ACID tables that _could_ be converted to ACID tables by the post hive
      'strictmigration' process.  This will dramatically change the characteristics of the table and affect the integration
      methods used by Spark SQL, direct HDFS access, etc..  Many of which will 'fail' and/or 'corrupt' the newly converted
      Hive ACID table.\n

      -- We recommend that you proactively force these tables to 'EXTERNAL' tables (with purge characteristics) to prevent this
      conversion.  Changing tables to ACID, post migration is much easier than trying to revert converted ACID tables back
      to non-ACID tables.\n

      -- Tables that are managed and aren't converted by this program, will be converted to 'EXTERNAL/purge' when they don't
      meet all the criteria for the conversion, regardless.  Using the 'action' elements of this report simply ensures
      you are in control of the migration process.\n
      
      -- These conversions need to happen BEFORE the tables are accessed in CDP.  Generally, this is done AFTER the upgrade
      to CDP and BEFORE the tables are turned over to the end users.  In some cases, due to the volume of changes needed,
      the conversion can be run before the upgrade.  BUT the conversion WILL have an impact on table behavior.  Convert tables
      in the legacy environment will NOT honor the 'purge' characteristics stated by the 'external.table.purge' TBLPROPERTIES.
      So be advised of this caveat if you choose to run these conversions BEFORE the upgrade.\n
      
      -- The report includes the tables 'created_date' in the comment line above the 'ALTER'.  You can use:\n
      --    grep -A1 'created date: yyyy-MM' managed_upgrade_2_acid.sql | grep 'ALTER TABLE' > beeline_run.sql\n
      --         OR
      --    grep 'ALTER TABLE' managed_upgrade_2_acid.sql | sort -k3 > beeline_run.sql\n
      -- to target certain tables first and run this post upgrade to make them available asap.\n
      
      -- Consider breaking the file into multiple parts to increase throughput and reduce runtime.\n
      -- The statement are meant to be run against 'hive'.  Use 'beeline -f <run.sql>'\n
      
      -- NOTE: This process CAN and SHOULD be run after the upgrade and after all the ALTER scripts have been run to ensure
      --       you haven't missed any conversion.\n\n"
      record:             "-- This table 'could' be migrated to an ACID table unless changed.\n
                          -- Recommend forcing the manual conversion to ensure table isn't inadvertently migrated.\n
                          -- created date: %3$s\n
                          ALTER TABLE `%1$s`.`%2$s` SET TBLPROPERTIES('EXTERNAL'='TRUE', 'external.table.purge'='true');"
  - type:                     "dbSet"
    id:                       9
    active:                   false
    displayName:              "Hive ACID tables missing 'bucketing_version'='2' tblproperties"
    queryDefinitionReference: "/hive_u3_queries.yaml"
    errorDescription:         "Issues"
    successDescription:       "SQL Conversion Script"
    errorFilename:            "acid_bucketing_update_issues.txt"
    successFilename:          "acid_bucketing_update.sql"
    dbListingQuery:           "db_tbl_count"
    listingColumns:           [ "db_name" ,"tbl_name" , "tbl_id"]
    pathsListingQuery:        "acid_bucket_version"
    hsmmElements:
      databaseField:          "db_name"
      tableField:             "tbl_name"
    skipCommandCheck:
      displayName:            "Hive ACID tables missing 'bucketing_version'='2' tblproperties"
      title:                  "-- Hive ACID tables missing 'bucketing_version'='2' tblproperties  (v.${Implementation-Version})\n"
      note:                   "-- This report will show the ACID tables that don't contain the required 'bucketing_version=2' tblproperties configuration.\n
      -- Without this setting, you will have issues access/reading/processing the upgraded ACID table.\n
      -- We recommend that you run the script to add this configuration value to the tables.\n\n"
      record:             "ALTER TABLE `%1$s`.`%2$s` SET TBLPROPERTIES('bucketing_version'='2');"
